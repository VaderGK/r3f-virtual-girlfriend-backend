STRUKTURA
📁 audios
│   ├── api_0.json
│   ├── api_0.mp3
│   ├── api_0.wav
│   ├── api_1.json
│   ├── api_1.mp3
│   ├── api_1.wav
│   ├── intro_0.json
│   ├── intro_0.wav
│   ├── intro_1.json
│   ├── intro_1.wav
│   ├── message_0.json
│   ├── message_0.mp3
│   ├── message_0.wav
│   ├── message_1.json
│   ├── message_1.mp3
│   ├── message_1.wav
│   ├── message_2.json
│   ├── message_2.mp3
│   └── message_2.wav
│
📁 src
│   ├── 📁 routes
│   │   ├── chatRoutes.js
│   │   ├── statusRoutes.js
│   │   └── ttsRoutes.js
│   │
│   ├── 📁 services
│   │   ├── lipSyncService.js
│   │   ├── openaiService.js
│   │   ├── ttsCartesia.js
│   │   ├── ttsElevenLabs.js
│   │   └── ttsService.js
│   │
│   ├── 📁 utils
│   │   ├── execUtils.js
│   │   ├── fileUtils.js
│   │   └── index.js
│   │
│   └── index.js
│
├── .env
├── .env.example
├── .gitignore
├── package.json
├── README.md
└── server.js



// server.js
//version 1.0.0
import express from 'express';
import dotenv from 'dotenv';
import cors from 'cors';

import indexRoutes from './src/index.js'; // Importujemy router z src/index.js
import statusRoutes from './src/routes/statusRoutes.js';

dotenv.config(); 

const app = express();
const PORT = process.env.PORT || 8080;

app.use(cors());
app.use(express.json());

// Używamy zaimportowanych routerów
app.use('/', indexRoutes);
app.use('/api/status', statusRoutes);

app.listen(PORT, () => {
    console.log(`Virtual Girlfriend listening on port ${PORT}`);
});

// package.json
//version 1.0.0
{
  "name": "r3f-virtual-girlfriend-backend",
  "version": "1.0.0",
  "description": "Backend for the virtual girlfriend app",
  "main": "server.js",
  "type": "module",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js --ext js",
    "postinstall": "apt-get update && apt-get install -y ffmpeg unzip && curl -L -o /tmp/rhubarb.zip https://github.com/DanielSWolf/rhubarb-lip-sync/releases/download/v1.13.0/Rhubarb-Lip-Sync-1.13.0-Linux.zip && unzip /tmp/rhubarb.zip -d /usr/local/bin/ && chmod +x /usr/local/bin/rhubarb && /usr/local/bin/rhubarb --version || echo 'Rhubarb nie działa'"
  },
  "author": "EFEKT.ai",
  "dependencies": {
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "openai": "^4.26.0",
    "node-fetch": "^3.3.1"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  }
}

// src/index.js
// version 1.0.0
import express from 'express';
const router = express.Router();
import chatRoutes from './routes/chatRoutes.js';
import ttsRoutes from './routes/ttsRoutes.js';
import { checkDependencies } from './utils/execUtils.js';

checkDependencies();

router.get("/", (req, res) => res.send("Hello World!"));

// Używamy routerów zdefiniowanych w innych plikach
router.use('/', chatRoutes);
router.use('/', ttsRoutes);

export default router;

// src/routes/chatRoutes.js
// version 1.0.0
import express from 'express';
import { generateChatResponse } from '../services/openaiService.js';

const router = express.Router();

router.post("/chat", async (req, res) => {
    try {
        const response = await generateChatResponse(req.body);
        res.send(response);
    } catch (error) {
        console.error("❌ Błąd podczas obsługi żądania czatu:", error);
        res.status(500).json({ error: "Błąd podczas generowania odpowiedzi OpenAI." });
    }
});

export default router;

// src/routes/statusRoutes.js
// version 1.0.0
import express from 'express';
const router = express.Router();

router.get("/", (req, res) => res.json({ status: "ok", message: "Backend działa poprawnie!" }));

export default router;

// src/routes/ttsRoutes.js
// version 1.0.0
import express from 'express';
const router = express.Router();
import { getVoices } from '../services/ttsService.js';


router.get("/voices", async (req, res) => {
    try {
        const voices = await getVoices();
        res.send(voices);
    } catch (error) {
        console.error("❌ Błąd pobierania głosów:", error);
        res.status(500).json({ error: "Błąd pobierania głosów ElevenLabs." });
    }
});

export default router;

// src/services/lipSyncService.js
// version 1.0.0
import { execCommand } from '../utils/execUtils.js';
import { promises as fs } from 'fs';
import { fileURLToPath } from 'url';
import path from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const lipSyncMessage = async (messageIndex) => {
    console.log(`🎶 Rozpoczęto lipSyncMessage: messageIndex=${messageIndex}`);

    const time = new Date().getTime();
    const mp3FileName = `audios/message_${messageIndex}.mp3`; // Zakładamy MP3
    const wavFileName = `audios/message_${messageIndex}.wav`;
    const jsonFileName = `audios/message_${messageIndex}.json`;

    console.log(`🔍 lipSyncMessage: mp3FileName="${mp3FileName}", wavFileName="${wavFileName}", jsonFileName="${jsonFileName}"`);


    try {
        // 1. Konwersja MP3 na WAV
        console.log(`🔄 Converting MP3 to WAV: ${mp3FileName} -> ${wavFileName}`);
        const ffmpegCommand = `ffmpeg -y -i ${mp3FileName} ${wavFileName}`;
        console.log(`⚙️ Wykonuję polecenie: ${ffmpegCommand}`);

        await execCommand(ffmpegCommand);
        console.log(`✅ Conversion done in ${new Date().getTime() - time}ms`);

        // 2. Generowanie lip sync za pomocą Rhubarb
        console.log(`👄 Generating lip sync data: ${wavFileName} -> ${jsonFileName}`);
        const rhubarbPath = '/usr/local/bin/Rhubarb-Lip-Sync-1.13.0-Linux/rhubarb'; // ✅ Poprawiona ścieżka
        const rhubarbCommand = `${rhubarbPath} -f json -o ${jsonFileName} ${wavFileName} -r phonetic`;
        console.log(`⚙️ Wykonuję polecenie: ${rhubarbCommand}`);

        await execCommand(rhubarbCommand);
        console.log(`✅ Lip sync done in ${new Date().getTime() - time}ms`);

        // 3. Odczyt pliku JSON
        console.log(`📖 Odczytuję plik JSON: ${jsonFileName}`);
        const lipSyncData = await readJsonTranscript(jsonFileName);

        if (!lipSyncData) {
            console.warn(`⚠️  Brak danych lip sync w pliku: ${jsonFileName}`);
        }

        console.log(`✔️ Dane lip sync odczytane pomyślnie`);
        return lipSyncData;
    } catch (error) {
        console.error(`❌ Błąd podczas generowania lip sync dla wiadomości ${messageIndex}:`, error);
        return null;
    }
};

const readJsonTranscript = async (file) => {
    try {
        console.log(`📄 Próba odczytu pliku JSON: ${file}`);
        const data = await fs.readFile(file, "utf8");
        const jsonData = JSON.parse(data);
        console.log(`✅ Plik JSON odczytany i sparsowany pomyślnie`);
        return jsonData;
    } catch (error) {
        console.error(`❌ Błąd odczytu pliku JSON: ${file}`, error);
        return null;
    }
};

export { lipSyncMessage }; 

// src/services/openaiService.js
// version 1.0.0
import OpenAI from 'openai';
import dotenv from 'dotenv';
import { generateSpeech } from './ttsService.js';
import { lipSyncMessage } from './lipSyncService.js'; // Załóżmy, że masz lipSyncService
import { promises as fs } from 'fs';

dotenv.config();

const ttsMode = process.env.TTS_MODE || 'eco'; // Domyślnie tryb ECO
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const audioFileToBase64 = async (file) => {
    try {
        const data = await fs.readFile(file);
        return data.toString("base64");
    } catch (error) {
        console.error(`❌ Błąd odczytu pliku audio: ${file}`, error);
        return null;
    }
};

export const generateChatResponse = async (body) => {
    console.log("🔵 Rozpoczęto generateChatResponse", { body });
    const { message: userMessage, user_browser_language: userBrowserLanguage = "en" } = body;

    if (!userMessage) {
        throw new Error("Brak wiadomości w żądaniu.");
    }

    if (!process.env.ELEVEN_LABS_API_KEY || !openai.apiKey) { // Poprawione sprawdzenie OpenAI API key
        throw new Error("Brak kluczy API OpenAI i ElevenLabs.");
    }

    const systemPrompt = `
        You are Liliana - a virtual girlfriend. You will always reply with a JSON array of messages. With a maximum of 3 messages.
        Each message has a text, facialExpression, and animation property.
        The different facial expressions are: smile, sad, angry, surprised, funnyFace, and default.
        The different animations are: Talking_0, Talking_1, Talking_2, Crying, Laughing, Rumba, Idle, Terrified, Angry) properties.

        Your response language should be determined based on the following priority:
        1. If the last user message is at least 15 characters long, detect its language and respond in that language.
        2. If the last two messages from the user were in the same language, respond in that language.
        3. Otherwise, respond in the language set in ${userBrowserLanguage}.

        Ensure that your language detection is accurate and do not switch languages unnecessarily.
    `;

    try {
        const completion = await openai.chat.completions.create({
            model: "gpt-4o-mini",
            max_completion_tokens: 1000,
            temperature: 0.7,
            response_format: { type: "json_object" },
            messages: [
                { role: "system", content: systemPrompt },
                { role: "user", content: userMessage },
            ],
        });

        let messages = JSON.parse(completion.choices[0].message.content);
        if (messages.messages) {
            messages = messages.messages;
        }

        console.log("🟢 Odpowiedź OpenAI:", { messages });
        console.log(`⚙️ Tryb TTS: ${ttsMode}`);

        if (ttsMode === 'pro') {
            // ✅ Tryb PRO: Wywołaj TTS równocześnie (Promise.all)
            await Promise.all(messages.map(async (message, i) => {
                try {
                    console.log(`➡️ Przetwarzanie wiadomości ${i}: "${message.text}"`);
                    const fileExtension = "mp3"; // Zmieniono na stale mp3
                    const fileName = `audios/message_${i}.${fileExtension}`;

                    const text = message.text.trim();

                    console.log(`🔍 [PRO] Generowanie audio dla wiadomości ${i}: "${text}"`);

                    const audioFile = await generateSpeech(text, fileName);
                    console.log(`🎵 Wygenerowano plik audio: ${audioFile}`);

                    if (!audioFile) {
                        console.error(`❌ [PRO] Nie udało się wygenerować audio dla wiadomości ${i}`);
                        return; // Przejdź do następnej iteracji
                    }

                    // ✅ Sprawdzenie, czy plik istnieje
                    try {
                        await fs.access(fileName);
                        console.log(`✅ [PRO] Plik istnieje: ${fileName}`);
                    } catch (error) {
                        console.error(`❌ [PRO] Plik NIE został zapisany: ${fileName}`);
                        return; // Przejdź do następnej iteracji
                    }

                    const lipSyncData = await lipSyncMessage(i);

                    if (!lipSyncData) {
                        console.warn(`⚠️  [PRO] Brak danych lip sync dla wiadomości ${i}`);
                        return;
                    }

                    const audioBase64 = await audioFileToBase64(fileName);

                    if (!audioBase64) {
                        console.error(`❌ [PRO] Błąd konwersji audio do base64 dla wiadomości ${i}`);
                        return;
                    }


                    message.lipsync = lipSyncData;
                    message.audio = audioBase64;
                } catch (error) {
                    console.error(`❌ [PRO] Błąd podczas przetwarzania wiadomości ${i}:`, error);
                }
            }));
        } else {
            // ✅ Tryb ECO: Wywołaj TTS sekwencyjnie (pętla for...of)
            for (const [i, message] of messages.entries()) {
                try {
                    console.log(`➡️ Przetwarzanie wiadomości ${i}: "${message.text}"`);
                   const fileExtension = "mp3"; // Zmieniono na stale mp3
                    const fileName = `audios/message_${i}.${fileExtension}`;

                    const text = message.text.trim();

                    console.log(`🔍 [ECO] Generowanie audio dla wiadomości ${i}: "${text}"`);

                    const audioFile = await generateSpeech(text, fileName);
                    console.log(`🎵 Wygenerowano plik audio: ${audioFile}`);

                    if (!audioFile) {
                        console.error(`❌ [ECO] Nie udało się wygenerować audio dla wiadomości ${i}`);
                        continue; // Przejdź do następnej wiadomości
                    }

                    // ✅ Sprawdzenie, czy plik istnieje
                    try {
                        await fs.access(fileName);
                        console.log(`✅ [ECO] Plik istnieje: ${fileName}`);
                    } catch (error) {
                        console.error(`❌ [ECO] Plik NIE został zapisany: ${fileName}`);
                        continue; // Przejdź do następnej wiadomości
                    }

                    const lipSyncData = await lipSyncMessage(i);
                    if (!lipSyncData) {
                        console.warn(`⚠️  [ECO] Brak danych lip sync dla wiadomości ${i}`);
                        continue;
                    }


                    const audioBase64 = await audioFileToBase64(fileName);

                    if (!audioBase64) {
                        console.error(`❌ [ECO] Błąd konwersji audio do base64 dla wiadomości ${i}`);
                        continue;
                    }

                    message.lipsync = lipSyncData;
                    message.audio = audioBase64;
                } catch (error) {
                    console.error(`❌ [ECO] Błąd podczas przetwarzania wiadomości ${i}:`, error);
                }
            }
        }

        return { messages };

    } catch (error) {
        console.error("❌ Błąd OpenAI:", error);
        throw new Error("Błąd podczas generowania odpowiedzi OpenAI.");
    }
};


// src/services/ttsCartesia.js
// version 1.0.0
import fetch from 'node-fetch';
import fs from 'fs/promises';
import dotenv from 'dotenv';
dotenv.config();

export const generateSpeechCartesia = async (text, fileName) => {
    console.log(`🎧 [Cartesia] Rozpoczęto generateSpeechCartesia: text="${text}", fileName="${fileName}"`);

    try {
        const url = "https://api.cartesia.ai/tts/bytes";
        console.log(`🌐 [Cartesia] Wywołuję API Cartesia: ${url}`);

        const requestBody = JSON.stringify({
            model_id: "sonic",
            transcript: text,
            voice: {
                mode: "id",
                id: "575a5d29-1fdc-4d4e-9afa-5a9a71759864"
            },
            output_format: {
                container: "mp3",
                bit_rate: 128000,
                sample_rate: 44100
            },
            language: "pl"
        });

        const response = await fetch(url, {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                "X-API-Key": process.env.CARTESIA_API_KEY,
                "Cartesia-Version": "2024-06-10",
            },
            body: requestBody,
        });

        console.log(`✉️  [Cartesia] Otrzymano odpowiedź z API Cartesia: ${response.status} ${response.statusText}`);

        if (!response.ok) {
            console.error(`❌ [Cartesia] Błąd API Cartesia: ${response.status} - ${response.statusText}`);
            try {
                const errorData = await response.json();
                console.error("📝 [Cartesia] Szczegóły błędu Cartesia:", errorData);
            } catch (e) {
                console.error("❌ [Cartesia] Błąd parsowania JSON z odpowiedzi Cartesia:", e);
                const errorText = await response.text();
                console.error("❌ [Cartesia] Treść odpowiedzi Cartesia (nie-JSON):", errorText);
            }
            return null;
        }

        const audioBuffer = await response.arrayBuffer();
        await fs.writeFile(fileName, Buffer.from(audioBuffer));
        console.log(`✅ [Cartesia] Plik audio poprawnie zapisany: ${fileName}`);
        return fileName;
    } catch (error) {
        console.error("❌ [Cartesia] Błąd połączenia z Cartesia API:", error);
        return null;
    }
};

// src/services/ttsElevenLabs.js
// version 1.0.0
import fetch from 'node-fetch';
import fs from 'fs/promises';
import dotenv from 'dotenv';
dotenv.config();

const voiceID = "XrExE9yKIg1WjnnlVkGX";

export const generateSpeechElevenLabs = async (text, fileName) => {
    try {
        const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceID}`, {
            method: "POST",
            headers: { "Content-Type": "application/json", "xi-api-key": process.env.ELEVEN_LABS_API_KEY },
            body: JSON.stringify({
                text, model_id: "eleven_multilingual_v2",
                voice_settings: { stability: 0.5, similarity_boost: 0.5, style: 0.0, use_speaker_boost: true }
            })
        });

        if (!response.ok) throw new Error(`Błąd API ElevenLabs: ${response.status}`);

        const audioBuffer = await response.arrayBuffer();
        await fs.writeFile(fileName, Buffer.from(audioBuffer));
        return fileName;
    } catch (error) {
        console.error("❌ Błąd ElevenLabs:", error);
        return null;
    }
};

// src/services/ttsService.js
// version 1.0.0
import { generateSpeechElevenLabs } from './ttsElevenLabs.js';
import { generateSpeechCartesia } from './ttsCartesia.js';
import dotenv from 'dotenv';
import fs from 'fs/promises';

dotenv.config();

let defaultProvider = process.env.DEFAULT_TTS_PROVIDER || "elevenlabs";

export const generateSpeech = async (text, fileName) => {
    console.log(`🎤 Rozpoczęto generateSpeech: text="${text}", fileName="${fileName}"`);
    console.log(`⚙️ Aktualny defaultProvider: ${defaultProvider}`);

    try {
        //await fs.unlink(fileName).catch(() => { }); // Pomijamy usuwanie pliku na początku

        if (defaultProvider === "elevenlabs") {
            console.log(`🗣️ Wywołuję generateSpeechElevenLabs: text="${text}", fileName="${fileName}"`);
            const result = await generateSpeechElevenLabs(text, fileName);
            console.log(`✔️ generateSpeechElevenLabs zwróciło: ${result}`);
            if (!result) {
                console.warn("⚠️  generateSpeechElevenLabs nie powiodło się. Przełączam na Cartesia...");
                defaultProvider = "cartesia";
                console.log(`🗣️ Wywołuję generateSpeechCartesia: text="${text}", fileName="${fileName}"`);
                return await generateSpeechCartesia(text, fileName);
            }
            return result;
        } else {
            console.log(`🗣️ Wywołuję generateSpeechCartesia: text="${text}", fileName="${fileName}"`);
            const result = await generateSpeechCartesia(text, fileName);
            console.log(`✔️ generateSpeechCartesia zwróciło: ${result}`);
            if (!result) {
                console.warn("🔄 generateSpeechCartesia nie powiodło się. Przełączam na ElevenLabs...");
                defaultProvider = "elevenlabs";
                console.log(`🗣️ Wywołuję generateSpeechElevenLabs: text="${text}", fileName="${fileName}"`);
                return await generateSpeechElevenLabs(text, fileName);
            }
            return result;
        }
    } catch (error) {
        console.error("❌ Błąd w generateSpeech:", error);
        return null;
    }
};

export async function getVoices() {
    try {
        const response = await fetch("https://api.elevenlabs.io/v1/voices?show_legacy=true", {
            headers: { "xi-api-key": process.env.ELEVEN_LABS_API_KEY },
        });

        if (!response.ok) {
            console.error("❌ Błąd pobierania głosów:", response.status, response.statusText);
            return [];
        }

        const data = await response.json();
        return data.voices || [];
    } catch (error) {
        console.error("❌ Błąd połączenia z ElevenLabs API:", error);
        return [];
    }
}
import fetch from 'node-fetch';


// src/utils/execUtils.js
// version 1.0.0
import { exec as childProcessExec } from "child_process";

export function checkDependencies() {
    exec("ffmpeg -version", (error, stdout) => {
        if (error) console.error("🚨 FFmpeg NIE jest zainstalowany!");
        else console.log("✅ FFmpeg działa:\n", stdout);
    });

    const rhubarbPath = "/usr/local/bin/rhubarb";
    exec("rhubarb --version", async (error, stdout) => {
        if (error) {
            console.warn("🚨 Rhubarb NIE jest zainstalowany! Pobieranie...");
            try {
                await execCommand(`curl -L -o ${rhubarbPath} https://github.com/DanielSWolf/rhubarb-lip-sync/releases/latest/download/rhubarb-linux`);
                await execCommand(`chmod +x ${rhubarbPath}`);
                console.log("✅ Rhubarb został pobrany.");
            } catch (installError) {
                console.error("❌ Nie udało się pobrać Rhubarb!", installError);
            }
        } else console.log("✅ Rhubarb działa:\n", stdout);
    });
}


export const execCommand = (command) => {
    return new Promise((resolve, reject) => {
        childProcessExec(command, (error, stdout, stderr) => {
            if (error) {
                console.warn(`❌ Błąd podczas wykonywania polecenia: ${command}`);
                reject(error);
                return;
            }
            console.log(`✅ Wykonano polecenie: ${command}`);
            resolve(stdout ? stdout : stderr);
        });
    });
};

// Funkcja exec do wykonywania poleceń systemowych
export const exec = (command, callback) => {
    childProcessExec(command, callback);
};

// src/utils/fileUtils.js
// version 1.0.0
import fs from 'fs/promises';

export const readJsonTranscript = async (file) => {
  try {
    const data = await fs.readFile(file, "utf8");
    return JSON.parse(data);
  } catch (error) {
    console.error(`❌ Błąd odczytu pliku JSON: ${file}`, error);
    return null;
  }
};